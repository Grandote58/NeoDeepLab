![PRACTICA](https://github.com/Grandote58/NeoDeepLab/blob/main/Clase008/assets/PRACTICA.png)


# **üß™ Pr√°ctica: Denoising Autoencoder para Detecci√≥n de Fraude Bancario**

## üéØ Objetivo general

Implementar, entrenar y evaluar un **Denoising Autoencoder** aplicado a transacciones bancarias (dataset de fraude con tarjetas de cr√©dito), con el fin de:

1. **Modelar el comportamiento normal** de las transacciones bajo presencia de ruido.
2. **Distinguir ruido aleatorio** de **anomal√≠as estructuradas** (fraude) a partir del **error de reconstrucci√≥n**.

## üéØ Metas de aprendizaje

Al finalizar la pr√°ctica, el estudiante ser√° capaz de:

1. Cargar y explorar un **dataset financiero abierto** de fraude con tarjetas de cr√©dito.
2. Identificar y **simular ruido** en las caracter√≠sticas de las transacciones.
3. Implementar un **Denoising Autoencoder** en Keras/TensorFlow 2.x que aprenda a ‚Äúlimpiar‚Äù ruido de transacciones normales.
4. Analizar el **error de reconstrucci√≥n** como indicador de anomal√≠a y derivar de √©l una regla de decisi√≥n.
5. Evaluar el desempe√±o de la t√©cnica mediante m√©tricas est√°ndar (confusion matrix, precision, recall, F1, ROC-AUC, PR curve).



> üîß **Antes de empezar en Colab**
>  Men√∫: **Entorno de ejecuci√≥n ‚Üí Cambiar tipo de entorno de ejecuci√≥n ‚Üí Acelerador por hardware: GPU (opcional pero recomendado)**.

# **üß± SECCI√ìN 0 ‚Äî Librer√≠as y configuraci√≥n inicial**

```python
# ============================================
# SECCI√ìN 0: IMPORTACI√ìN DE LIBRER√çAS Y SETUP
# ============================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import (confusion_matrix, classification_report,
                             precision_score, recall_score, f1_score,
                             roc_auc_score, roc_curve, precision_recall_curve)

import tensorflow as tf
from tensorflow.keras import layers, models

# Configurar estilo de gr√°ficas
plt.style.use("seaborn-v0_8")

# Mostrar versi√≥n de TensorFlow
print("Versi√≥n de TensorFlow:", tf.__version__)

# Semillas para reproducibilidad
SEED = 42
np.random.seed(SEED)
tf.random.set_seed(SEED)
```

# **üì• SECCI√ìN 1 ‚Äî Carga del dataset abierto y vista general**

Usaremos el dataset p√∫blico **Credit Card Fraud** alojado por TensorFlow (misma estructura que el cl√°sico de Kaggle).

```python
# ============================================
# SECCI√ìN 1: DESCARGA Y CARGA DEL DATASET
# ============================================

# Descarga del dataset (datos abiertos)
!wget -q https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv -O creditcard.csv

# Cargar CSV en un DataFrame
data = pd.read_csv("creditcard.csv")

print("Dimensiones del dataset:", data.shape)
print("\nPrimeras filas:")
display(data.head())

print("\nInformaci√≥n del dataset:")
print(data.info())
```

# **üìä SECCI√ìN 2 ‚Äî Exploraci√≥n de datos (EDA) y ruido impl√≠cito**

### üîπ 2.1 Distribuci√≥n de la variable Clase (0 vs 1)

```python
# ============================================
# SECCI√ìN 2: EXPLORACI√ìN DE DATOS
# ============================================

class_counts = data["Class"].value_counts()
print("Distribuci√≥n de clases:\n", class_counts)

fraud_pct = class_counts[1] / class_counts.sum() * 100
print(f"\nPorcentaje de fraudes: {fraud_pct:.4f}%")

plt.figure(figsize=(6,4))
class_counts.plot(kind="bar", color=["tab:blue", "tab:red"])
plt.title("Distribuci√≥n de clases (0 = Normal, 1 = Fraude)")
plt.xlabel("Clase")
plt.ylabel("N√∫mero de transacciones")
plt.xticks(rotation=0)
plt.grid(axis="y")
plt.show()
```

> üí° Reflexi√≥n: el **fraude es muy raro** ‚Üí el ruido/varianza de comportamiento normal es mucho mayor en volumen que los fraudes.

### üîπ 2.2 Estad√≠sticos de variables clave: Time y Amount

```python
print("\nDescripci√≥n estad√≠stica de 'Time' y 'Amount':")
display(data[["Time", "Amount"]].describe().T)

# Histograma de Amount
plt.figure(figsize=(6,4))
plt.hist(data["Amount"], bins=50)
plt.title("Distribuci√≥n del monto (Amount)")
plt.xlabel("Amount")
plt.ylabel("Frecuencia")
plt.grid(True)
plt.show()

# Histograma de Time
plt.figure(figsize=(6,4))
plt.hist(data["Time"], bins=50)
plt.title("Distribuci√≥n de 'Time'")
plt.xlabel("Time (segundos desde la primera transacci√≥n)")
plt.ylabel("Frecuencia")
plt.grid(True)
plt.show()
```

> üí¨ Comentario t√©cnico: la variabilidad (ruido) natural en montos y tiempos es alta. El autoencoder debe aprender qu√© combinaciones de variables son ‚Äúnormales‚Äù a pesar de esa variaci√≥n.

# **üßº SECCI√ìN 3 ‚Äî Preprocesamiento y definici√≥n de ruido**

### Idea clave de la t√©cnica

- Trabajaremos con las variables num√©ricas (`Time`, `V1`‚Äì`V28`, `Amount`).
- **Entrenaremos un Denoising Autoencoder solo con transacciones normales**:
  - **Entrada**: transacci√≥n **normal + ruido gaussiano sint√©tico**.
  - **Salida (target)**: transacci√≥n normal original (sin ruido).
- De este modo, el modelo aprende a **filtrar ruido aleatorio** y reconstruir el ‚Äúpatr√≥n limpio‚Äù de comportamiento normal.
- Cuando vea transacciones fraudulentas o muy extra√±as, las tratar√° como ‚Äúruido estructurado‚Äù dif√≠cil de limpiar ‚Üí **error de reconstrucci√≥n alto**.

### üîπ 3.1 Separar X e y

```python
# ============================================
# SECCI√ìN 3: PREPROCESAMIENTO
# ============================================

X = data.drop("Class", axis=1)
y = data["Class"]

print("Shape X:", X.shape)
print("Shape y:", y.shape)
```

### üîπ 3.2 Escalado de caracter√≠sticas

Usamos `StandardScaler` para que el autoencoder trabaje con distribuciones centradas y comparables.

```python
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print("Shape X_scaled:", X_scaled.shape)

X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)
print("\nDescripci√≥n estad√≠stica tras escalar:")
display(X_scaled_df.describe().T.head(10))
```

### üîπ 3.3 Separaci√≥n Normal vs Fraude

```python
normal_mask = (y == 0)
fraud_mask = (y == 1)

X_normal = X_scaled[normal_mask]
X_fraud  = X_scaled[fraud_mask]

print("Transacciones normales:", X_normal.shape[0])
print("Transacciones fraude  :", X_fraud.shape[0])
```

# **üå´Ô∏è SECCI√ìN 4 ‚Äî Generaci√≥n expl√≠cita de ruido (Gaussiano) y justificaci√≥n**

### Concepto de ruido en esta pr√°ctica

- **Ruido aleatorio (sint√©tico)**: peque√±as perturbaciones gaussianas que afectan las variables pero no cambian la ‚Äúnaturaleza‚Äù de la transacci√≥n (sigue siendo normal).
- **Fraude**: no lo generamos artificialmente; ya est√° en el dataset. Es una **anomal√≠a estructurada**, no solo ruido peque√±o.

### üîπ 4.1 Funci√≥n para a√±adir ruido gaussiano a transacciones normales

```python
# ============================================
# SECCI√ìN 4: GENERACI√ìN DE RUIDO
# ============================================

def add_gaussian_noise(X, mean=0.0, std=0.05):
    """
    A√±ade ruido gaussiano a una matriz X.
    
    X: np.array de forma (n_muestras, n_features)
    mean: media del ruido
    std: desviaci√≥n est√°ndar del ruido
    """
    noise = np.random.normal(loc=mean, scale=std, size=X.shape)
    X_noisy = X + noise
    return X_noisy

# Aplicar ruido sobre transacciones normales
X_normal_noisy = add_gaussian_noise(X_normal, mean=0.0, std=0.1)

print("Shape X_normal:", X_normal.shape)
print("Shape X_normal_noisy:", X_normal_noisy.shape)
```

### üîπ 4.2 Comparar distribuciones con y sin ruido (una feature de ejemplo)

```python
# Elegimos arbitrariamente una columna, por ejemplo 'Amount'
col_name = "Amount"
col_idx = list(X.columns).index(col_name)

plt.figure(figsize=(8,4))
plt.hist(X_normal[:, col_idx], bins=50, alpha=0.7, label="Original")
plt.hist(X_normal_noisy[:, col_idx], bins=50, alpha=0.7, label="Con ruido")
plt.title(f"Distribuci√≥n de {col_name}: original vs con ruido gaussiano")
plt.xlabel("Valor escalado")
plt.ylabel("Frecuencia")
plt.legend()
plt.grid(True)
plt.show()
```

> üß† **Lectura t√©cnica:** el ruido gaussiano simula peque√±as perturbaciones naturales, errores de medici√≥n o variaciones menores. El autoencoder aprender√° a ignorar ese ruido y reconstruir la forma base de la transacci√≥n.

# **üîÄ SECCI√ìN 5 ‚Äî Train/Test para el Denoising Autoencoder**

Entrenaremos el modelo **solo con normales**:

- **Entrada de entrenamiento**: `X_normal_noisy`
- **Target de entrenamiento**: `X_normal`

Para evaluaci√≥n de fraude:

- Usaremos un **test combinado** con normales y fraudes **sin ruido sint√©tico** (el ‚Äúruido‚Äù relevante ahora ser√° el propio comportamiento an√≥malo del fraude).

```python
# ============================================
# SECCI√ìN 5: SPLIT TRAIN / TEST PARA EL DENOISING
# ============================================

# Dividir las transacciones normales (originales y con ruido) en train/test
X_train_clean, X_val_clean, X_train_noisy, X_val_noisy = train_test_split(
    X_normal, X_normal_noisy, test_size=0.2, random_state=SEED
)

print("X_train_clean:", X_train_clean.shape)
print("X_val_clean  :", X_val_clean.shape)
print("X_train_noisy:", X_train_noisy.shape)
print("X_val_noisy  :", X_val_noisy.shape)

# Conjunto de test final para detecci√≥n de fraude:
# normales (limpios) + fraudes (limpios)
X_test_combined = np.vstack([X_val_clean, X_fraud])
y_test_combined = np.hstack([np.zeros(len(X_val_clean)), np.ones(len(X_fraud))])

print("\nX_test_combined:", X_test_combined.shape)
print("y_test_combined:", y_test_combined.shape)
```

# **üß† SECCI√ìN 6 ‚Äî Definici√≥n del Denoising Autoencoder**

Arquitectura simple pero suficiente:

- Entrada: vector de caracter√≠sticas con ruido (noisy).
- Encoder: Dense 64 ‚Üí 32 ‚Üí bottleneck 16.
- Decoder: Dense 32 ‚Üí 64 ‚Üí salida lineal (reconstrucci√≥n limpia).

```python
# ============================================
# SECCI√ìN 6: MODELO DENOISING AUTOENCODER
# ============================================

input_dim = X_train_noisy.shape[1]
encoding_dim = 16

input_layer = layers.Input(shape=(input_dim,), name="input_noisy")

# Encoder
x = layers.Dense(64, activation='relu', name="enc_dense1")(input_layer)
x = layers.Dense(32, activation='relu', name="enc_dense2")(x)
latent = layers.Dense(encoding_dim, activation='relu', name="latent")(x)

# Decoder
x = layers.Dense(32, activation='relu', name="dec_dense1")(latent)
x = layers.Dense(64, activation='relu', name="dec_dense2")(x)
output_layer = layers.Dense(input_dim, activation='linear', name="output_clean")(x)

denoising_autoencoder = models.Model(inputs=input_layer,
                                     outputs=output_layer,
                                     name="denoising_autoencoder_fraude")

denoising_autoencoder.summary()
```

> üéØ **T√©cnica subrayada:**
>  Estamos entrenando un modelo que ‚Äúaprende a quitar ruido‚Äù de transacciones normales.
>  El fraude no es visto en el entrenamiento ‚Üí se comporta como ‚Äúruido estructural‚Äù en test.

# **‚öôÔ∏è SECCI√ìN 7 ‚Äî Compilaci√≥n y entrenamiento (con validaci√≥n)**

```python
# ============================================
# SECCI√ìN 7: COMPILACI√ìN Y ENTRENAMIENTO
# ============================================

denoising_autoencoder.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss="mse"
)

EPOCHS = 30
BATCH_SIZE = 256

history = denoising_autoencoder.fit(
    X_train_noisy, X_train_clean,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    shuffle=True,
    validation_data=(X_val_noisy, X_val_clean),
    verbose=1
)
```

### üîπ 7.1 Curvas de p√©rdida (train vs val)

```python
plt.figure(figsize=(6,4))
plt.plot(history.history["loss"], label="P√©rdida entrenamiento")
plt.plot(history.history["val_loss"], label="P√©rdida validaci√≥n")
plt.title("Curva de p√©rdida - Denoising Autoencoder")
plt.xlabel("√âpocas")
plt.ylabel("Loss (MSE)")
plt.legend()
plt.grid(True)
plt.show()
```

> üí° *Si hay divergencia fuerte entre loss y val_loss, probablemente haya sobreajuste o ruido excesivo.*

# **üßæ SECCI√ìN 8 ‚Äî Evaluaci√≥n como detector de anomal√≠as (fraudes)**

Ahora usamos el modelo entrenado para reconstruir **X_test_combined** (normales + fraudes, sin ruido sint√©tico).

### üîπ 8.1 Reconstrucci√≥n y error MSE

```python
# ============================================
# SECCI√ìN 8: RECONSTRUCCI√ìN Y ERROR EN TEST
# ============================================

# Reconstrucci√≥n con el autoencoder (entrada: datos limpios)
reconstructions_test = denoising_autoencoder.predict(X_test_combined)

# Error MSE por muestra
mse_test = np.mean(np.power(X_test_combined - reconstructions_test, 2), axis=1)

print("Shape mse_test:", mse_test.shape)
print("Primeros 10 MSE:", mse_test[:10])
```

### üîπ 8.2 Distribuci√≥n global de errores

```python
plt.figure(figsize=(6,4))
plt.hist(mse_test, bins=50)
plt.title("Distribuci√≥n del error de reconstrucci√≥n - Test combinado")
plt.xlabel("MSE")
plt.ylabel("Frecuencia")
plt.grid(True)
plt.show()
```

### üîπ 8.3 Separar error en normales vs fraudes

```python
mse_normal_test = mse_test[y_test_combined == 0]
mse_fraud_test  = mse_test[y_test_combined == 1]

print("MSE Normal - media:", np.mean(mse_normal_test), "mediana:", np.median(mse_normal_test))
print("MSE Fraude - media:", np.mean(mse_fraud_test), "mediana:", np.median(mse_fraud_test))

plt.figure(figsize=(6,4))
plt.hist(mse_normal_test, bins=50, alpha=0.7, label="Normal")
plt.hist(mse_fraud_test,  bins=50, alpha=0.7, label="Fraude")
plt.title("MSE: Normales vs Fraudes (Denoising AE)")
plt.xlabel("MSE")
plt.ylabel("Frecuencia")
plt.legend()
plt.grid(True)
plt.show()
```

> üß† **Lectura t√©cnica de la t√©cnica:**
>
> - Para el modelo, el fraude se comporta como un ‚Äúruido‚Äù que no sabe limpiar.
> - El Denoising AE sabe eliminar ruido gaussiano peque√±o, pero el fraude altera la estructura multimodal de los datos ‚áí mayor error de reconstrucci√≥n.

# **üîê SECCI√ìN 9 ‚Äî Selecci√≥n del umbral (t√©cnica de detecci√≥n)**

Aqu√≠ puntualizamos c√≥mo se genera la detecci√≥n.

### T√©cnica de decisi√≥n (muy importante)

- Definimos un **umbral de error** `T`.
- Si `MSE(transacci√≥n) ‚â• T` ‚Üí la transacci√≥n se clasifica como **an√≥mala/fraude**.
- Si `MSE(transacci√≥n) < T` ‚Üí la transacci√≥n se clasifica como **normal**.

Elegiremos T como un percentil alto del MSE de las transacciones normales de test (`mse_normal_test`).

```python
# ============================================
# SECCI√ìN 9: UMBRAL PARA DETECCI√ìN DE FRAUDE
# ============================================

threshold = np.percentile(mse_normal_test, 97)  # por ejemplo, percentil 97
print("Umbral de anomal√≠a (percentil 97 de normales):", threshold)
```

## üßÆ SECCI√ìN 10 ‚Äî Clasificaci√≥n final y m√©tricas

### üîπ 10.1 Etiquetas predichas

```python
# ============================================
# SECCI√ìN 10: CLASIFICACI√ìN Y M√âTRICAS
# ============================================

y_pred = (mse_test >= threshold).astype(int)

print("Primeras 20 predicciones:", y_pred[:20])
print("Primeros 20 valores reales:", y_test_combined[:20])
```

### üîπ 10.2 Matriz de confusi√≥n y m√©tricas clave

```python
cm = confusion_matrix(y_test_combined, y_pred)
print("Matriz de confusi√≥n:\n", cm)

tn, fp, fn, tp = cm.ravel()
print(f"\nTN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}")

print("\nReporte de clasificaci√≥n:")
print(classification_report(y_test_combined, y_pred, digits=4))

precision = precision_score(y_test_combined, y_pred)
recall = recall_score(y_test_combined, y_pred)
f1 = f1_score(y_test_combined, y_pred)

print(f"Precision: {precision:.4f}")
print(f"Recall   : {recall:.4f}")
print(f"F1-score : {f1:.4f}")
```

### üîπ 10.3 ROC-AUC y curva ROC (usando el MSE como score)

```python
roc_auc = roc_auc_score(y_test_combined, mse_test)
print("ROC-AUC (Denoising AE, MSE como score):", roc_auc)

fpr, tpr, _ = roc_curve(y_test_combined, mse_test)

plt.figure(figsize=(6,4))
plt.plot(fpr, tpr, label=f"ROC (AUC = {roc_auc:.4f})")
plt.plot([0,1], [0,1], "k--", label="Random")
plt.title("Curva ROC - Denoising Autoencoder para fraude")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.grid(True)
plt.show()
```

### üîπ 10.4 Curva Precision‚ÄìRecall

```python
prec_vals, rec_vals, _ = precision_recall_curve(y_test_combined, mse_test)

plt.figure(figsize=(6,4))
plt.plot(rec_vals, prec_vals)
plt.title("Curva Precision-Recall - Denoising AE")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.grid(True)
plt.show()
```


![pie](https://github.com/Grandote58/NeoDeepLab/blob/main/Clase008/assets/pie.png)

